[abstract]
== Abstract

Polynomial approximations of the sine and cosine functions are given. The
polynomials approximating the sine function use only odd powers, all
coefficients for even powers are zero. Likewise, the cosine approximations use
only even coefficients. The error function which was minimized to derive the
coefficients is the maximum of the relative or absolute error over the "first"
quarter of one period of the sine function. The polynomials can be evaluated
quickly using Horner's scheme.

ifdef::env-github[]
:toc-title:  
== Table of Contents
endif::env-github[]

toc::[]

== Approximations

include::approximation-sections.adoc[]

[appendix]
== Summary of Polynomial Degree vs. Max. Error

[#error-summary-plot]
image::error-summary-plot.svg[error-summary-plot,width=100%,pdfwidth=100%,scaledwidth=100%]

[appendix]
== Why approximate sin(2Ï€x) et al. this way?

=== Context

The approximations are provided with DSP applications in mind. Calculating sines
and cosines with low memory footprint, only floating-point multiply-adds and no
more precision than needed can improve performance significantly in some cases.

The interval [-1/2, 1/2] can be folded with the function f(x) = max(min(x, 1/2 -
x), -1/2 - x)) such that a polynomial p(x), which approximates sin(2Ï€x) over
[-1/4, 1/4] (which is true for odd polynomials fitted over [0, 1/4]) can be
evaluated over [-1/2, 1/2] as p(f(x)). To approximate sin(2Ï€x) over [-âˆž, âˆž], one
can evaluate p(f(x - âŒŠx + 1/2âŒ‹)). Similar folding functions can be used for
cosine approximations and obviously, both the polynomials approximating sine as
well as those approximating cosine can be used as building block for either sine
or cosine over [-âˆž, âˆž].

=== What sorts of intervals and sine function variations to optimize over

I chose to approximate sin(2Ï€x) for 0 < x < 1/4 and cos(2Ï€x) for 0 < x < 1/4.
sin(2Ï€x a) for 0 < x < 1/(4a) and any a âˆˆ â„ is already covered then, since it is
trivial to transform the coefficients accordingly and the optimum approximation
coefficients would be the same, except for that transformation. Similarly, (a
sin(2Ï€x)) for any a âˆˆ â„ does not gain anything. This includes -sin and -cos in
particular. sin(2Ï€x + a) for 0 < x < 1/4 and any a âˆˆ â„ is not generally
sufficient since one needs to approximate at least a complete quarter of one
period from an extremum to a zero of the sine function to be able to calculate
all sine output values from that approximation. At the same time, approximating
over a bigger interval than a quarter of a period is either redundant or would
(probably) need unnecessarily many coefficients (although I have not tested or
proven that last assumption).

=== What sorts of polynomial coefficient sets to use

After having found a number of optimal approximations for given sets of
coefficients between a~0~ and a~7~ and by minimizing the p-Norm of the error for
p âˆˆ {2, 4, 6, 8, âˆž}, it became empirically clear that the most efficient method
of achieving the lowest error using the fewest coefficients is to only use
coefficients with an odd index (a~1~, a~3~, a~5~, a~7~, â€¦) for sine and only
coefficients with an even index (a~0~, a~2~, a~4~, a~6~, â€¦) for cosine. This is
somewhat expected since sine and cosine are odd and even functions,
respectively. Other coefficients tend to improve the approximation less than
using the next higher-order odd (or even) coefficient while needing the same
number of multiplications and additions.  Generally, the results always showed
the least error for a given number of nonzero coefficients when using only odd
or even coefficients. Interestingly, the approximations of the sine function for
a given number of odd coefficients were usually marginally or even substantially
better than the approximations of the cosine function for an equal number of
even coefficients. I have not attempted to prove these results or found
something like a proof, so they may not generally hold true for higher degree
polynomials, other values of p, or other norms in general, but the trend was
definitely visible in the data.  Evaluating a polynomial with n coefficients
a~i~ where i = 2 j + 1 and j âˆˆ {0, â€¦, n-1} (having a sine approximation in mind)
takes merely one multiplication more than evaluating such a polynomial where i =
2 j (having a cosine approximation in mind) when using Horner's scheme.

Some of the discussed findings are reflected by <<Garrett2012>> and
<<samhocevar2018>>.

=== More design variables

Other things to consider when finding such sine approximations include choosing
whether the points sin(0) == 0 and sin(2Ï€ 1/4) == 1 should be matched perfectly
by the polynomial, whether one wants to minimize relative, absolute, or some
other error, choosing the norm one wants to minimize and choosing the numerical
precision of the coefficients one wants to derive.

Having "exact" output values at the zeros and extrema of the sine function might
come in handy in certain situations, but it comes at the cost of a higher
maximum error over the interval.

When trying to find approximations whose errors are near a floating-point
epsilon, one should obviously minimize the relative error. For an approximation
that does not focus on reaching some floating-point precision, minimizing the
absolute error may be more appropriate, depending on the application.

Of the p-Norms, p -> âˆž (the Max-Norm) does not increase the mean error
significantly compared to an approximation using, say, the 2-norm, while the
maximum error decreases substantially, which is often desirable.

=== Methods to find optimal approximations

One can of course formulate the problem as an optimization problem and do an
exhaustive (or "smart") search of the parameter space. I was able to do this
fairly successfully with polynomials of low degrees. An advantage of this method
was that I did not need to implement any fancy algorithms or use any algebra to
get special polynomials where some coefficients a~i~ == 0. That being said, this
method is of course very naive. It can reach exponential runtime and I would
highly discourage it.

The Remez algorithm <<Remez1934>> can be used to minimize the Max-Norm.

<<Garrett2012>> describes an algorithm for minimizing the 2-Norm of the absolute
error. I did not check whether this can be generalized to other p-Norms, but I
would assume so.

=== Summary

So basically, my goal was to find polynomials, whose even coefficients are equal
to zero, with a minimum error on the interval [0, 1/4] when approximating
sin(2Ï€x). Similarly, I found approximations for cos(2Ï€x) using only even
coefficients. I chose the Max-Norm to measure the error over the approximation
interval. I found approximations both for minimal relative error and for minimal
absolute error. The value at x == 0 is then fixed by definition for the sine
approximations, but I left other extrema (and zeros of the cosine
approximations) open for optimization. The coefficients were found with
extremely high precision and then rounded to 36 digits before evaluation.

I am sure something like this or quite similar approximations have been done
thousands of times before, but I had a hard time finding a collection like in
this document, so I chose to make one myself.

=== Implementation

Although there were a couple of options, I ended up using the Julia language to
do the actual approximation finding. I had tried with the Wolfram Language's
Function Approximation Package and the C++ Boost Libraries among others, but
nothing seemed to work quite the way I needed it to. I'm sure that I can blame
myself in part. It's always kind of funny and sad how much time one can waste on
such little stupid things, always thinking that the solution is basically right
around the next corner while not anticipating the next problem awaiting you
instead. Anyhowâ€¦ ðŸ˜„ There is a Julia package called Remez.jl <<simonbyrne2018>>,
which only worked in Julia 0.6 at the time of writing. Here's the code I finally
wrote to get my approximations:

[source,julia]
----
using Remez
setprecision (2000)

for sine in[true, false]
  for relative in[true, false]
    for degree in 0 : 16
      filename = "outfiles/"*(sine?"" : "co")*"sine-"*lpad(degree, 2,
        0)*(relative?"-relative.txt" : "-absolute.txt")
      println ("Computing: "*(sine?"S" : "Cos")*"ine, "*(relative?"relative" :
        "absolute")*" error, degree $degree. -> $filename")
      if sine
        out = ratfn_minimax ((x) -> sin (sqrt (x))/(sqrt (x)), (BigFloat (1. e -
          318), BigFloat (pi*pi/4.)), degree, 0, (relative?(x, y) -> 1./y
          : (x, y) -> sqrt (x)))
      else
        out = ratfn_minimax ((x) -> cos (sqrt (x)), (BigFloat (1. e - 318),
          BigFloat (pi*pi/4.)), degree, 0, (relative?(x, y) -> 1./y : (x, y)
          -> 1.))
      end

      open (filename, "w") do f
        write (f, string (out))
      end
    end
  end
end
----

[bibliography]
== Bibliography

- [[[Garrett2012]]] Charles Kristopher Garrett, "Fast Polynomial Approximations
  to Sine and Cosine", Technical Report (2012)
- [[[samhocevar2018]]]
  https://github.com/samhocevar/lolremez/wiki/Tutorial-3-of-5:-changing-variables-for-simpler-polynomials
  (accessed 2018-09-12)
- [[[simonbyrne2018]]] https://github.com/simonbyrne/Remez.jl (accessed
  2018-09-12)
- [[[Remez1934]]] E. Ya. Remez, "Sur la dÃ©termination des polynÃ´mes
  d'approximation de degrÃ© donnÃ©e", Comm. Soc. Math. Kharkov 10, 41 (1934); "Sur
  un procÃ©dÃ© convergent d'approximations successives pour dÃ©terminer les
  polynÃ´mes d'approximation, Compt. Rend. Acad. Sc. 198, 2063 (1934)

